```{r todo-tables-general, echo=FALSE, eval=FALSE}
prolly use fewer distinct example data sets, especially in joins. this can be done after everything's in and organized throughout the full tutorial
```

# Tables {#tables}

This chapter covers navigating tabular data with R. The core syntax is (\@ref(dt-syntax)):

    DT[where, select|update|do, by] # SQL verbs
    DT[i, j, by]                    # R function arguments

It reads as:

1. Subset using `i`, then
1. Group using `by`, then
1. Do `j`


## Essential packages

This section covers packages that help me work more efficiently, to the point where I regard them as essential. They'll be used throughout the rest of this document. They don't have any dependencies on other packages, and I expect they'll be available as long as R is. It will be assumed in subsequent sections that these libraries have been attached:

```{r loadem, warning=FALSE}
library(magrittr)
library(data.table)
```

### The magrittr package {#magrittr}

[Magrittr](https://cran.r-project.org/package=magrittr) introduces syntax with "pipes," improving readability by unnesting function calls:

```{r magrittr-calls, eval=FALSE}
# we can do
x %>% f %>% g %>% h
# instead of 
h(g(f(x)))
```

In addition, it allows more compact function definitions (\@ref(function-writing)):

```{r magrittr-funs}
fun = . %>% .^2 %>% sum %>% sqrt
fun(1:3)
```

Despite these advantages, magrittr can slow down code, so avoid using it anywhere where speed might be an issue.

To install, just use CRAN:

```{r inst-mgr, eval=FALSE}
install.packages('magrittr')
```

The name is a pun on [RenÃ© Magritte's pipe painting](https://en.wikipedia.org/wiki/The_Treachery_of_Images). To get a handle on the package, I would start with the package [vignette](https://CRAN.R-project.org/package=magrittr/vignettes/magrittr.html).

### The data.table package {#data-table}

[Data.table](http://r-datatable.com) offers a variant of the `data.frame` class for data frames (seen in \@ref(data-frames)), optimized for fast sorted and grouped operations and enhanced with cleaner syntax. Section \@ref(dt-class) introduces this `data.table` class. 

The package also bundles in a variety of other functionality:

- ITime and IDate date and time classes
- `fread` and `fwrite` for fast reading and writing of delimited files
- `dcast` and `melt` for reshaping tables

Because these disparate features are bundled together in data.table, we don't have to load more packages and worry about their complicated dependencies or namespace conflicts (\@ref(namespaces)).

#### Installation

Again, CRAN can be used:

```{r inst-dt, eval=FALSE}
install.packages('data.table')
```

However, the package is under active development, with new features available only in the development version. To install it, follow [the instructions from the package wiki](https://github.com/Rdatatable/data.table/wiki/Installation):

```{r inst-dt-dev, eval=FALSE}
remove.packages("data.table")
install.packages("data.table", type = "source",
  repos = "http://Rdatatable.github.io/data.table")
```

If using Windows, you'll need to first install [Rtools](https://cran.r-project.org/bin/windows/Rtools/) (requiring administrator privileges), as explained in the last link.

#### Getting started

The official vignettes for the package are a great way to start with the package. See them on the [wiki](http://r-datatable.com/Getting-started) or with 

```{r, eval=FALSE}
browseVignettes(package="data.table")
```

(Beware that as of March 2017, the website vignettes are somewhat out-of-date.) The website also includes links to other useful materials (an online course, presentation materials, blog posts). Before using the package, I started by reading some slides and the FAQ in full.



## The data.table class {#dt-class}

Data.tables extend the data frame class introduced in \@ref(data-frames). 

This section reviews basic operations seen in the last chapter (inspecting, slicing, extracting), before discussing how `data.table` extends `data.frame`.

### Inspecting {#dt-inspect}

Consider the `quakes` data set (with more info in `?quakes`):

```{r dt-view-pre, echo=1}
quakeDT = data.table(quakes)
quakeDT
```

By default, data.tables with over 100 rows will print in the compressed form seen above, showing just the first and last five rows. To globally change how many rows are needed, use `options(datatable.print.nrows = n)`. To override it a single time, use `print` with `nrows=`. I usually set `nrows=Inf` to print all rows:

```{r dt-print, eval=FALSE}
quakeDT %>% print(nrow = Inf)
```

To browse a table in a new window, `View` for data frames (\@ref(view)) again works:

```{r dt-view, eval=FALSE}
quakeDT %>% View
# or if using RStudio
quakeDT %>% utils::View()
```

To inspect the structure of a data.table, we can again use `str` (\@ref(str)):

```{r dt-str}
quakeDT %>% str
```

Here, we see the class of each column along with the first few values. In addition, there is an obscure `".internal.selfref"` attribute, which we can ignore except when saving or loading the table from disk (\@ref(dt-saveload)). Other attributes will sometimes also show up here, related to optimizing the performance of ordered or grouped queries on the table (\@ref(keys-indices)).

In the code above, we could use syntax like `View(quakeDT)` instead of `quakeDT %>% View`, but I often find the latter handier, since it's easy to insert intermediate steps, like `quakeDT %>% head(10) %>% View`. Also, while `%>%` is slow, it's not going to matter for tasks like browsing data.

### Slicing {#dt-slicing}

A slice of a data.table is a smaller data.table formed by subsetting rows, columns, or both, analogous to vector and matrix slices seen in \@ref(slicing) and \@ref(slicing-matrix). The remaining columns will retain attributes from the full table (class, levels for factors, etc.).

The syntax is familiar from vectors and matrices:

```{r dt-slicing-pre}
# example data
DT = data.table(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
```
```{r dt-slicing}
DT[1:2, ]
DT[, "z"]
DT[-3, c("y", "z")]
```

One difference with matrices is that when only slicing rows, we can skip the comma:

```{r dt-slicing-rows}
DT[1:2]
```

Other differences come into play when subsetting columns programmatically, where we need either a `..` prefix...

```{r dt-slicing-cols-vec, error=TRUE}
keep_cols = c("y", "z")
DT[, keep_cols]                  # error
DT[, ..keep_cols]                # use this instead
```

... or `with=FALSE` if programming inline:

```{r dt-slicing-cols-program}
DT[, letters[25:26]]             # no error, but doesn't print what we want
DT[, letters[25:26], with=FALSE] # use this instead
```

These requirements are a quirk of the more flexible `DT[...]` syntax that supports far more than taking slices, as discussed in \@ref(dt-syntax).

```{block2 dt-colnumbers, type='rmd-caution'}
**Use column names, not numbers.** Subsetting by hard-coded column number, like `DT[, 2:3]` or `cols = 2:3; DT[, ..cols]` works but is discouraged. Column numbers can easily change in the course of writing or updating a script, invalidating column number references in a way that will be annoying to debug. See the first answer in `vignette("datatable-faq")` for a deeper discussion.
```

```{block2 dt-dfbrackafrak, type='rmd-details'}
**How data frame slicing works.** This is really getting in the weeds, since I suggest not using data frames at all, but you'll see two major differences if you do. First, `DT[, "z"]` will extract the `z` column instead of taking a slice thanks to data frames' `drop=TRUE` default (which also came up regarding matrices in the last chapter). Second, `DT[1:2]` will slice the first two *columns* instead of the first two rows, thanks to the fact that data frames (and data.tables) are lists and the no-comma usage of `[` triggers list slicing.
```

`head`, `tail` and the special empty and missing-data slices (all seen in the last chapter) work by row:

```{r dt-slice-headtailspecial}
head(DT, -3)    # top, removing last 3
tail(DT, 2)     # bottom 2
DT[0L]          # empty 
DT[NA_integer_] # missing
```

The package also offers `first` and `last`, which are simply special cases of `head` and `tail`.

Fancier slicing methods, like `DT[x > "b", .(y, z)]`, will be introduced with the rest of the `DT[...]` syntax in \@ref(dt-syntax).

There are some syntactical shortcuts for slicing to a set of columns in `j`; see \@ref(program-cols).

### Extracting columns

Since data.tables (and data frames) are lists, we can extract columns like `DT$z` or `DT[["z"]]`. 

With the full `DT[...]` syntax (\@ref(dt-syntax)), it is easy to extract a column for a limited set of rows, like `DT[x > "b", z]`.

### Extensions to the data.frame class

The first thing to note is that data.tables *are* data frames, so any function that works on a data frame will work fine with data.tables, too (even if the data.table package is not installed or attached).

#### Modification in-place

In contrast with the rest of R, data.tables are primarily modified "in-place" (or "by reference"), which can be much more efficient. As a first example, we can switch the class of a data frame to data.table:

```{r setDT-pre}
# example data
DF = data.frame(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
```
```{r setDT}
str(DF)
setDT(DF)
str(DF)
```

We did not use `=` or `<-` to assign the result of `setDT`; the function simply altered `DF` in-place. All of data.table's functions named like `set*` do this.

```{block2 dt-byref, type='rmd-details'}
**R's copy-on-modify.** Understanding the contrast between modification in-place and base R's modification rules (often called "copy-on-modify") probably requires some familiarity with C (see threads on [stackoverflow](http://stackoverflow.com/questions/15759117/what-exactly-is-copy-on-modify-semantics-in-r-and-where-is-the-canonical-source) or [the mailing list](http://r.789695.n4.nabble.com/Confused-about-NAMED-td4103326.html) if interested). I wouldn't worry about it except to note that many base R operations make copies, which is costly in terms of RAM and computing time; while those operations' data.table counterparts do not have this problem.
```

#### Factor vs character columns {#dt-factorvchar}

Another difference is that `data.frame(...)` reads string input as a `factor` categorical variable (\@ref(factors)), as seen in the `str` output above; while `data.table(...)` reads it as character:

```{r DT}
DT = data.table(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
str(DT)
```

Data frames' preference for factors may still bite when reading files in with `read.table` from base R instead of `fread` from data.table (\@ref(fread)).

#### Extended syntax for `DT[...]`\ {#dt-syntax}

The third major difference is the extension of the `DT[...]` syntax to support more than simple slices (like `DF[i,j]`, where `i` and `j` as indices, covered in \@ref(dt-slicing)). It offers SQL-style syntax that is cleaner, particularly for by-group operations:

    # (pseudocode)
    DT[where, select|update|do, by] # SQL verbs
    DT[i, j, by]                    # R function arguments

This should be read as a command to take a sequence of steps:

1. Subset using `i`
1. Group using `by`
1. Do `j`

We can use column names as barewords, and even form expressions in terms of columns:

```{r DT-ex}
DT[x > "b", sum(y), by=z]
```

Typically, the `i` and `j` arguments are called by position, as seen here; while `by=` is called by name. 

Whenever `j` evaluates to a list, the output will be a new data.table:

```{r DT-exdot}
DT[x > "b", .(n_bigy = sum(y > 2), n_smally = sum(y <= 2)), by=z]
```

```{block2 dt-dot, type='rmd-caution'}
**The `.()` convenience function.** Inside many arguments of `DT[...]`, we can use the shorthand `.()`, which stands for `list()`.
```

The use of `by=` is very similar to a `for` loop (\@ref(for-loops)) over subsets, but it is better in a few important ways: 

- We don't have to manually construct and keep track of some "split-up data" list to iterate over.
- Fast by-group functions are used when available (see `?GForce`)
- We don't have to define the task in `j` as a function of prespecified variables -- we can just use any columns of the data.table.
- We don't have to worry about intermediate variables (like `case` in the next example) contaminating the global environment.

There are some syntactical shortcuts for writing a list of columns in `by=`; see \@ref(program-cols).

#### An example

The task in `j` can really be anything. Just as a demonstration of its power, here's how it can be used for saving per-group plots to a PDF:

```{r dt-bydemo, cache=TRUE}
# note: this code will save to your current working directory
# type getwd() and read ?setwd for details

bwDT = data.table(MASS::birthwt)

pdf(file="birthweight_graphs.pdf")
bwDT[, {
  case = sprintf("high #visits? = %s, smoking? = %s", .BY$high_vis, .BY$smoke)
  
  cat("Handling", case, "...\n")
  plot(
    age ~ lwt, 
    main = case
  )  
}, by=.(high_vis = ftv >= 1, smoke)]
dev.off()
```

Plotting graphs is beyond the scope of this document, but the example above should make intuitive sense after reading the docs for each object, `?MASS::birthwt`, `?pdf`, et al. For each group, we're formatting a string with `sprintf`; printing it with `cat`; and saving a plot. The special symbol `.BY` is a list containing the per-group values of the `by=` variables. Formatting and printing strings will be covered more in \@ref(strings).

```{block2 dt-logicolsub, type='rmd-caution'}
**Filtering on logical columns.** `DT[z]` and `DT[!z]` will give errors, for design reasons related to the syntax for joins (\@ref(dt-joins)). To get around this, always wrap the column in parentheses: `DT[(z)]` and `DT[!(z)]`.
```

```{block2 dt-byfloats, type='rmd-caution'}
**Grouping on floats.** Much of the computational and syntactical magic of the package comes from grouping rows together with the `by=` argument. To group on a floating-point variable, however, is just asking for trouble, [for the usual numerical computing reasons](http://floating-point-gui.de/). Instead, always use characters, integers or factors. To discretise a continuous variable into bins, use `cut`.
```

```{block2 dt-verbose, type='rmd-details'}
**Verbose data.table messages.** To learn how data.table queries work, I recommend toggling the setting `options(datatable.verbose = TRUE)`. This option is similar to verbose output from an optimization call (reporting the value of the objective at each iteration, etc.). To only see verbose output for a single call, add `verbose = TRUE`. For example,`DT[x > "b", sum(y), by=z, verbose=TRUE]`.
```


## Aggregation

The mtcars data set (see `?mtcars`) has two categorical variables: 

- `am` for automatic (0) or manual (1) transmission; and 
- `vs` for v (0) or straight (1) engine shape. 

```{r agg-pre}
# example data
carsDT = data.table(mtcars, keep.rownames = TRUE)
# quick inspection
first(carsDT)
```

Suppose we want to compare the mean horsepower, `hp`, across these categories:

```{r agg}
carsDT[, .(mean_hp = mean(hp)), by=.(am, vs)]
```

So, we just write `j` of `DT[i,j,by]` as an expression to compute the summary statistic, optionally giving it a name by wrapping in `.(name = expression)`.

Section \@ref(dt-summary-stats) covers more options for exploring data with summary statistics; and \@ref(dcast) shows how to put this result in wide format (with, e.g., `am` on rows and `vs` on columns).

### Iterating over columns {#dt-lapply}

Now suppose we want to compare mean horsepower, weight and displacement:

```{r agg-sd, eval=-(1:2)}
carsDT[, .(mean_hp = mean(hp), mean_wt = mean(wt), mean_disp = mean(disp)), by=.(am, vs)]
# can be simplified to...
carsDT[, lapply(.SD, mean), by=.(am, vs), .SDcols = c("hp", "wt", "disp")]
```

So, we just write the relevant columns in `.SDcols` and refer to `.SD`, the ***S**ubset of **D**ata*. Within each `by=` group, the query has access to `.SD` with rows for that group and columns as specified in `.SDcols`. `.SD`, like `.BY` seen earlier, is a special symbol available in some arguments of `DT[...]`, documented at `?.SD`.

We can use `lapply` (a function designed for iterating over lists) here since `.SD` is a data.table, which is a list of column vectors (see \@ref(lapply-df)). The column names carry over to the result because `lapply` always carries over names. 

While dot notation `.SDcols=.(hp, wt, disp)` is not yet supported, there are a variety of convenience features for specifying `.SDcols`, covered in \@ref(program-cols).

```{block2 dt-avoidbyrow, type='rmd-caution'}
**"Aggregating" across columns.** One major red flag to look out for is the desire to "aggregate" columns by row, setting `by=1:nrow(DT)` and possibly using `unlist(.SD)` somewhere. Not only will this be incredibly slow, but it also suggests that the data is poorly organized, costing a lot of extra mental energy at every step. Section \@ref(structuring-data) explains some ways to format data better to avoid the need for this problematic approach.
```


## Modifying data {#dt-subassign}

Creating, editing and removing columns are all done using `:=` in `j` of `DT[i, j, by]`. This functionality operates in-place, in the sense that the underlying data stays in the same place, which is more efficient in terms of how much RAM and time is taken. See `vignette("datatable-reference-semantics")` for details.

```{block2, type='rmd-details'}
**What can be modified in-place?** The scope of in-place operations is currently limited to altering columns. Adding and removing rows in-place is not yet supported; it is harder to do in R, due to its column-oriented storage of tables (contrasting with database systems that store data rowwise for easy INSERT and DELETE queries).
```

### Creating columns {#dt-col-create}

```{r dt-add-pre, echo=1:2}
# example data
DT = data.table(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
DT
```
```{r dt-add}
# creating a column
DT[, u := 5:1][]
# creating multiple
DT[, `:=`(v = 2, w = 3L)][]
# creating with dynamic names
nms = c("a", "b", "c")
DT[, (nms) := .(1, 2, 3)][]
```

All of these tasks are performed in-place -- altering `DT` without making a new object. Usually, the results are not printed in the console; but they appear here because `[]` is "chained" onto the end of each task.

```{block2 modify-op, type='rmd-caution'}
**`:=` is the function *creation, modification and deletion* of columns.** This will be covered it more detail in subsequent sections, but is worth emphasising. In particular, this contrasts with Stata (which uses distinct verbs `gen`, `replace` and `drop`).
```

```{block2 modify-iter, type='rmd-details'}
**Iterative column creation.** The `` `:=`(...)`` syntax does *not* support iterative definitions like ``DT[, `:=`(W1 = u + y, W2 = W1^2)]``. One common workaround is ``DT[, `:=`(W1 = W1 <- u + y, W2 = W1^2)]``. This solution may not be intuitive for new R users, but the gist is: `W1 <- u + y` creates `W1` as an object in `DT[...]` and then returns its value, let's call it `v`. Now, `W2 = W1^2` can find `W1`, since it was created by `<-`; and `W1 = W1 <- u + y` simplifies to `W1 <- v`, where `v` is the return value of `W1 <- u + y`.
```

### Removing columns

```{r dt-remove}
nms = c("u", "v", "w", "a", "b", "c")
DT[, (nms) := NULL][]
```

A warning will print if we remove some columns that don't currently exist.

### Replacing entire columns

Data.table is careful about column types:

```{r dt-reptot, warning=TRUE}
DT[, a := 10L ][]  # Create a new column of integers
DT[, a := 21L ][]  # Replace it with another column of integers
DT[, a := 32.5 ][] # Replace it with a float
```

The warning in the last call is related to coercion of vector classes (\@ref(classes)).

```{block2 modify-typesafe, type='rmd-details'}
**Safeguards against accidental coercion.** The verbose warning printed above is typical for the package and an excellent feature. Running the final command, data.table knows that the `a` column is of integer type and sees that 32.5 is conspiciously *not an integer*. So it gives a warning when coercing 32.5 to an integer (to match `a`). 

Elsewhere in R, `a` would be coerced to match the float `32.5` -- probably not the behavior we want -- with no warning. That is, if we have `x <- c(21L, 22L)`, we can freely assign `x[1] <- 32.5`; and the same freedom (by which I mean "danger") is present even if `x` is a data frame column. For more on the issue, search online for "type safety."
```

If we want this assignment to work, we need to change `a`'s type by passing a full vector, as described in the warning:

```{r dt-reptot-class, warning=TRUE}
DT[, a := rep(32.5, .N) ][]
```

The coercion is done silently, but it can be made more visible by turning on `verbose`, which notes the "plonk" of a full vector replacement.

### Replacing columns conditionally {#dt-ifelse}

Conditional replacement here is analogous to a SQL UPDATE query or a replace if command in Stata. To illustrate, we will look again at a vectorized `if`/`else` assignment, mentioned in \@ref(ifelse):

```{r dt-ifelse-1, results="hide"}
DT[     , b := "Aardvark"] # initialize to baseline value
```
```{r dt-ifelse-2}
DT[y > 1, b := "Zebra"][] # replace based on a condition
```

This can also be done with chaining:

```{r dt-ifelse-chain, results="hide"}
DT[, b := "Aardvark"][y > 1, b := "Zebra"]
```

This chaining works because `DT[...][...]` is evaluated like `(DT[...])[...]` and the return value of the piece in parentheses is `DT` -- provided `j` has a `:=` statement.

```{block2 modify-chainwarn, type='rmd-caution'}
**Broken chains.** If `j` is not a `:=` statement, the return value is not the original data.table but rather a new one. Subsequent steps in the chain will not affect the starting table. So, after `DT[y < Inf, d := 1]` and `DT[y < Inf][, d := 2]`, what does the `d` column look like in `DT`? See the Exercise section of `vignette("datatable-reference-semantics")`; and the Note section of `` ?`:=` ``.
```

As we saw in the last section, partial replacement of a column will trigger a warning if the classes don't match:

```{r dt-ifelse-conflict, warning=TRUE, results="hide"}
DT[, b := "Aardvark"][y > 1, b := 111]
```

Another nice feature, similar to Stata, is reporting of the number of rows modified. This can be seen by turning `verbose` on:

```{r dt-ifelse-count}
DT[, b := "Aardvark"][y > 1, b := "Zebra", verbose = TRUE]
```

### Other in-place modifications

The data.table package has a few other tools for modifying table attributes in-place:

- The `set` function is another way of making assignments like `:=`.

- `setDT` and `setDF`, seen earlier, alter the class.

- `setorder` will sort the table by some or all of its columns.

- `setcolorder` changes the order in which columns are displayed.

- Indices and the key (explained in \@ref(keys-indices)) can be set with `setindex` and `setkey`, respectively.

- `setnames` will alter column names. For example, in \@ref(dt-lapply) we saw...

    ```{r modify-namesexpost-pre}
    carsDT[, lapply(.SD, mean), by=.(am, vs), .SDcols = c("hp", "wt", "disp")]
    ```

    ... and if we want to add the prefix `"mean_"` to the results, we can do

    ```{r modify-namesexpost}
    cols = c("hp", "wt", "disp")
    carsDT[, lapply(.SD, mean), by=.(am, vs), .SDcols = cols] %>% 
      setnames(cols, sprintf("mean_%s", cols)) %>% print
    ```
    
    The trailing `%>% print` command is used because, `setnames`, like `:=` and the other `set*` operators, does not print the table on its own. The string-formatter `sprintf` will be explained in \@ref(strings).


- `setattr` is a general function for altering attributes of a vector or other object (see `?attributes`). For example, if we have a factor column (encoding categorical data), we can change its "levels":

    ```{r dt-setlevels-pre}
    # example data
    fDT = data.table(fac = factor(c("A", "B"), levels = c("A", "B", "C")))
    ```
    ```{r dt-setlevels}
    levels(fDT$fac)
    fDT$fac %>% setattr("levels", c("A", "B", "Q"))
    levels(fDT$fac)
    ```


### Avoiding in-place modification

To create a new data.table starting from an existing table, use

```{r dt-copy, eval=FALSE}
DT2 = copy(DT1)
```

We have to use this instead of `DT2 = DT1` since with the latter we have only created a new "pointer" to the first table. I routinely make copies like this after reading in data (\@ref(fread)) so that I can back out where I tripped over something in the process of data cleaning.

Besides `DT2 = DT1`, `names1 = names(DT1)` is also unsafe, since the `names` function does not extract the column names as they are at a given time, but rather points at the names attribute, which can change as columns are modified or rearranged.

### Using in-place modification in functions

A user-written function (\@ref(function-writing)) like

```{r modify-fun}
f <- function(DT) DT[, newcol := 1111 ]
```

will act like `:=` and the `set*` functions, altering its input in-place. This can be useful, but requires extra caution. See the "`:=` for its side effect" section of `vignette("datatable-reference-semantics")` for discussion.


```{r todo-modify-ex, echo=FALSE, eval=FALSE}
### Exercises

use carsDT = data.table(mtcars, rn = TRUE), overwrite am with factor levels "automatic" and "manual"

```

## Joins {#dt-joins}

They are called "joins" or "merges."

### Equi joins

```{r dt-join-prea, echo=c(1:2,4L)}
# example data
a = data.table(id = c(1L, 1L, 2L, 3L, NA_integer_), t = c(1L, 2L, 1L, 2L, NA_integer_), x = 11:15)
a
b = data.table(id = 1:2, y = c(11L, 15L))
b
```

The idiom for a simple equi join is `x[i, on=.(...)]` or `x[i]` for short: 

```{r dt-join}
a[b, on=.(id)]
```

It is called an equi join since we are only getting matches where equality holds between the `on=` columns in the two tables.

Think of `x[i]` as using index table `i` to look up rows of `x`, in the same way an "index matrix" can look up elements of a matrix (\@ref(matrix-extract)). By default, we see results for every row of `i`, even those that are unmatched. 

Here are some more complicated examples:

```{r dt-join-more}
a[b, on=.(x = y)]
a[b, on=.(id, x = y)]
```

When merging on columns with different names, they must be written in `on=` like `x = y` where `x` is from the "left" table, and `y` from the "right" one. Because we are using `i` to lookup rows in `x`, the displayed column will have its name from `x` and its values from `i`.

A character vector also works in, e.g., `on=c("id", x = "y")`, making it easier to merge programmatically.

```{r todo-join-eff, eval=FALSE, echo=FALSE}
details re efficiency, citing "binary search vs vector scans" section of `vignette("datatable-keys-fast-subset")` and perhaps a benchmark
```

### Subset lookup

For browsing dynamic data (that grows over time), it is convenient and quick to use joins:

```{r dt-lookup}
a[.(1L), on=.(id)]
# or, as a function
ra <- function(my_id) a[.(my_id), on=.(id)] 
ra(2L)
```

Note that the index `i` is a list here, not a data.table. The link between the two is close, since data.tables are just lists of columns (\@ref(dt-lapply)). Any list passed in `i` will be treated the same as the analogous data.table with appropriate column names.

Subset browsing becomes even easier when keys are set so that `on=` can be skipped (\@ref(keys-indices)).

### Aggregating in a join

Looking again at the first join above, suppose we want to use `b` to find rows in `a` and then add up `a$x`. We'll do this by using `by=.EACHI`:

```{r join-byeachi}
a[b, on=.(id)]
a[b, on=.(id), sum(x), by=.EACHI]
```

It is called "each `i`" since the syntax is `x[i, ...]` and we are computing per row of the index `i`. I prefer to write the `on=` before the `j` (unlike the core syntax of `DT[i,j,by]`) since the `on=` merging columns are closely related to the `i` index.

If we tried summing `b$y` here, we would not get `11+11` for `id` 1:

```{r join-byeachi-confusing}
a[b, on=.(id)] # y shows up twice
a[b, on=.(id), .(sum(x), sum(y)), by=.EACHI] # we only get one y
```

This is because we are working by each row of `b`. For `id` 1 in `b`, `y` is a single value, `11`. So really there is no point in summing `y` or otherwise aggregating columns from `i` when using `by=.EACHI`.

### Updating in a join {#joins-update}

Continuing from the last example, we are computing `sum(x)` per row of `b`, so maybe we want to save the result as a column in `b`:

```{r join-byeachi-assign, echo=-4}
b[, sumx := 
    a[b, on=.(id), sum(x), by=.EACHI]$V1
]
# or
b[, sumx := 
    a[.SD, on=.(id), sum(x), by=.EACHI]$V1
]
b
```

```{block2 v1v2v3, type='rmd-caution'}
**Default names for `j` computations.** When the task in `j` evaluates to an unnamed list, default names `V1, V2, ...` are assigned. We can use these to extract a single computed column with `$V1`, `$V2`, etc. Since we can only extract one column at a time, we usually only create and extract a single column, auto-named `V1`.
```

On the other hand, we may want to take values from `b` and assign them to the larger table `a`:

```{r join-assign, echo=-2}
a[b, on=.(id), y := i.y ]
a
```

This sort of operation is very common when doing analysis with well-organized relational data, and will come up again in \@ref(structuring-data). It is essentially the same as a SQL UPDATE JOIN. It might also be called a "merge assign" (though I seem to be the only one using that term).

The `i.*` prefix in `i.y` indicates that we are taking the column from the `i` table in `x[i]`. We can similarly use an `x.*` prefix for columns from `x`. This helps to disambiguate if the same column names appear in both tables, and is particularly helpful with non-equi joins (\@ref(joins-nonequi)). I recommend always using `i.*` prefixes when copying columns in an update join.

```{block2 dt-updatejoin-beware, type='rmd-caution'}
**Beware multiple matches in an update join.** When there are multiple matches (\@ref(join-multimatch)), an update join will apparently only use the last one. [Unfortunately](https://github.com/Rdatatable/data.table/issues/2022), this is done silently. Try `b[a, on=.(id), x := i.x, verbose = TRUE ][]`. With `verbose` on, we see a helpful message about assignment "to 3 row subset of 2 rows."
```

### Self join to fill in missing levels

Consider the `a` data set from above:

```{r dt-join-CJ-pre, echo=1}
# example data
a = data.table(id = c(1L, 1L, 2L, 3L, NA_integer_), t = c(1L, 2L, 1L, 2L, NA_integer_), x = 11:15)
a
```

Now we want to "complete" the data set so that every `id` has a row for every time `t`:

```{r dt-join-CJ}
a[CJ(id = id, t = t, unique=TRUE), on=.(id, t)]
```

This is called a self join because we are using the table's own columns in `i` of `x[i, on=]`. The `CJ` function is a helper that takes all combinations of vectors, alternately called the Cartesian product or a "cross join." By applying `unique=TRUE`, we treat each vector *as a set* in the mathematical sense, only considering distinct values.

Notice that missing values (`NA`) in `i` are treated in the same way as other values.

### Handling matches {#join-matches}

For this section, we'll reset the tables:

```{r dt-mult-pre, echo=-c(3,5)}
# example data
a = data.table(id = c(1L, 1L, 2L, 3L, NA_integer_), t = c(1L, 2L, 1L, 2L, NA_integer_), x = 11:15)
a
b = data.table(id = 1:2, y = c(11L, 15L))
b
```

#### Handling multiply-matched rows {#join-multimatch}

In `a[b, on=.(id)]`, we are indexing by rows of `b` and so get matches for every row of `b`. By default, we get *all* matches in `a`, but this can be tweaked:

```{r dt-join-mult}
a[b, on=.(id), mult="first"]
```

Now each row of `b` only returns the first matching row (from the top) in `a`. Similarly, we could select `mult="last"`.

#### Handling unmatched rows

Flipping it around, if we use `a` to index `b`, we have some index rows from `a` that don't have any matches in `b`:

```{r dt-join-nomatch}
b[a, on=.(id)]
```

These unmatched rows still show up in the result, which is usually nice. However, this behavior can also be tweaked:

```{r dt-join-nomatch0}
b[a, on=.(id), nomatch=0]
```

Dropping unmatched elements of `i` is similar to filtering `_m == 3` in Stata.

```{block2 dt-merge-res, type='rmd-details'}
**Diagnostics for merges.** In Stata, joins report on how well they went -- did everything match? how many didn't match? The analogous question for an `x[i]` join is -- for each row of `i`, how many matches did we find in `x`? To see the answer, use `b[a, on=.(id), .N, by=.EACHI]`.
```
```{block2 dt-merge-stata, type='rmd-details'}
**Comparison with Stata.** Because `x[i]` uses `i` to look up rows in `x`, we are never looking ar rows that correspond to Stata's `_m == 1` -- that belong to `x` but are not matched by `i`.

There is another way of merging, `merge(b, a)`, that allows for `all.x` and `all.y`, resembling Stata's options, but I have never found any reason to use it.
```

#### Handling imperfect matches with rolling joins

Sometimes we want unmatched rows paired with the closest match occurring earlier or later in the table:

```{r im-rolling-im-rolling}
# target x
myxDT = list(myx = c(5L, 10L, 15L, 20L))

# exact match (equi-join)
a[myxDT, on=.(x = myx), .(i.myx, x.x)]
# nearest match
a[myxDT, on=.(x = myx), roll="nearest", .(i.myx, x.x)]
# upward match within 3
a[myxDT, on=.(x = myx), roll=-3, .(i.myx, x.x)]
```

Recall from \@ref(joins-update) that the `i.*` and `x.*` prefixes refer to where columns come from in `x[i]`.

When joining on multiple columns, the roll is taken on the last listed in `on=`.

The value of `roll=` refers to how much higher or lower the value of `x` can be and still qualify as a match. We add (up to) `roll` to the target row if necessary to find a match. So `roll = -3` means we would accept a `x` as far away as `x - 3 = myx`. 

### Non-equi joins {#joins-nonequi}

It is sometimes useful to match on a range of values. To do this, we explicitly name all columns in `i` and define inequalities in `on=`. Suppose we want to see, for every time `t` in 1..5, how many individuals were seen in the preceding three days:

```{r join-interval, echo=-2}
mDT = data.table(id = 1:3, x_dn = 10L, x_up = 13L)
a[mDT, on=.(id, x >= x_dn, x <= x_up), .(id, i.x_dn, i.x_up, x.x)]
```

So we are defining a range `x_dn` to `x_up` for each `id` and finding all matches of `x` within the range.

These could alternately be called "interval joins." For more on interval joins and subsets, see `?inrange`, `?foverlaps` and the [IRanges package](http://www.bioconductor.org/packages/IRanges/) that inspired these data.table tools.

### Shortcuts and tricks

When joining on a single character or factor column, the `.()` in `i` can be skipped:

```{r join-char, eval=1:2}
DT = data.table(id = c("A", "A", "B"), u = c(0, 1, 2), v = c(1, 4, 7))
DT["A", on=.(id)]
# instead of 
DT[.("A"), on=.(id)]
```

#### Setting keys and indices {#keys-indices}

If a table is always joined on the same column(s), these can be set as its "key." Setting the key of `x` sorts the table and allows for skipping `on=` during `x[i,on=]` joins. It also can have some performance benefits. See `?setkey`.

```{r join-setkey}
setkey(DT, id)
DT["A"]
```

This can be dangerous, however, since even if `i` has names, they are ignored:

```{r join-setkey-warn}
setkey(DT, u, v)
DT[.(v = 7, u = 2)]
```

No match is found here since `i=.(v = 7, u = 2)` is mapped to the key `.(u,v)` by position and not by name.

Most of the performance benefits of a key can also be achieved with an "index." Unlike a key, which sorts the table, an index simply notes the order of the table with respect to some columns. A table can have many indices, but only one key:

```{r join-indices, eval=1:2}
setindex(DT, id, u)
setindex(DT, id, v)
key(DT)
indices(DT, vectors=TRUE)
str(DT)
```

The performance benefits show up in most joins and some subsetting operations, too. Turn on `verbose` data.table messages to see when it is and is not kicking in. Also see `vignette("datatable-secondary-indices-and-auto-indexing")`; and regarding whether setting a key is important (as some old tutorials might say), see the package developer's [post](https://github.com/Rdatatable/data.table/issues/1232#issuecomment-131190268).

Keys and indices are destroyed whenever any of their columns are edited in a way that won't obviously preserve order.

```{block2 dt-wrongkeys, type='rmd-details'}
**Attribute verification.** Keys and indices are simply stored as attributes and may be invalid. For example, with `DT = data.table(id = 2:1, v = 2:1); setattr(DT, "sorted", "id")`, we've told the table that it is sorted by `id` even though it isn't. Some functions may check the validity of the key, but others won't, like `DT[.(1L)]`.
```

#### Anti joins

We also have the option of selecting *unmatched* rows:

```{r dt-join-notjoin}
a[!b, on=.(id)]
```

This "not join" or "anti join" returns all rows of `a` that are not matched by rows in `b`.

```{r todo-join-ex, eval=FALSE, echo=FALSE}
### Exercises

- use an anti join to assign to rows not matched in b

- initialize v := 0L. Then in an update self join, use mult= to tag the first row of each group with 1L.

- use a self non-equi join to count the number of observations within the last 90 days per id

```


## Input and output {#input-output}

This section covers reading tables from disk; basic cleaning of column formats; and writing to disk.

### File paths

Don't provide paths with backslashes, like `"C:\data\input.csv"`, since `\` is special character in R. My workaround is to use forward slashes or construct the path using `file.path`.

- For a list of files and folders, use `dir` or `list.files`. 
- For relative paths, `"."` is the current folder; `".."` navigates one level up; and `normalizePath` converts to an absolute path. 
- For the current path, use `getwd`; and to alter it, `setwd(new_path)`.
- To extract parts of a path, use, e.g., `file_ext` from the tools package (included in the base R installation).

For functions to manipulate files, see `?files` and `?dir.create`; and for file attributes, `?file.info`.

### `fread` to read delimited files {#fread}

To read tables from CSVs or other delimited formats, `fread` is quite fast and reliable:

```{r fread, eval=FALSE}
DT = fread("file.csv")
```

See `?fread` and [the wiki](https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread) for a discussion of features. I usually find that `fread` just works without any need for tweaks. However, it currently doesn't recognize date or time formats, which need to be handled after reading (\@ref(dates-times)).

### `rbindlist` to combine tables {#rbindlist-read}

To read and combine several tables with the same columns, `rbindlist` is the right tool. The short approach is:

```{r fread-list, eval=FALSE}
rbindlist(lapply(list.files(patt="csv$"), fread), id=TRUE)
# or...
list.files(patt="csv$") %>% lapply(fread) %>% rbindlist(id=TRUE)
```

The rest of this section covers the longer approach I recommend, using a table with one row per file:

```{r fread-list-long-pre}
# note: this code will save to your current working directory
# type getwd() and read ?setwd for details

# example data
set.seed(1)
for (i in 1:3) 
  fwrite(data.table(id = 1:2, v = sample(letters, 2)), file = sprintf("file201%s.csv", i))
```
```{r fread-list-long}
# First, identify the files wanted:
fileDT = data.table(fn = list.files(pattern="csv$"))

# Next, optionally parse the names for metadata using regex:
fileDT[, year := type.convert(sub(".*([0-9]{4}).*", "\\1", fn))]

# Finally construct a string file-ID column:
fileDT[, id := as.character(.I)][]
```

From here, read in the files as a new column:

```{r fread-list-long2}
fileDT[, contents := .(lapply(fn, fread))][]
```

Then, combine them for the final table:

```{r fread-list-long3}
DT = fileDT[, rbindlist(setNames(contents, id), idcol="file_id")]

# Perhaps add in metadata using an update join
DT[fileDT, on=.(file_id = id), year := i.year ][]
```

Results can be debugged by investigating individual files like `fileDT[year == 2012, contents[[1]]]`. The `setNames` function (\@ref(names)) is needed here since `rbindlist` uses names to populate the id column.

### Date and time columns {#dates-times}

Currently, `fread` does not recognize or translate time or date columns to native R formats. 

I recommend a couple packages, anytime and nanotime, for use when needed (more on which below). They have minimal dependencies and are maintained with an eye towards continued compatibility with the data.table package.

#### Date and time of day, separately

I usually handle this using the formats provided with data.table, `IDate` and `ITime`, for date and time of day:

```{r datetime-pre, echo=-3}
# example data
DT = data.table(id = 1:2, d = c("2017/01/21", "2017/01/22"), t = c("5:50", "13:50"))
DT
str(DT)
```
```{r datetime}
DT[, `:=`(
  d = as.IDate(d),
  t = as.ITime(t)
)][]
str(DT)
```

In the `str` output, we can see that ITime is stored as an integer, and the same is true of IDate. This is more efficient than a floating-point class in terms of storage and many table operations.

In this example, the `as.*` functions immediately recognize how to interpret the strings given. In many cases, however, we need to pass a format parameter, like `as.IDate("01/13/2013", "%m/%d/%Y")`. See `?strptime` and `?as.Date` for documentation on how to do this. Because it's hard to get the format right the first time, I usually test a format out like...

```{r datetime-fmttry, eval=FALSE}
DT[1L, as.IDate(d, "%Y%M%D")] 
```

This is similar to the `dryrun` option in Nick Cox's numdate package for Stata. 

Parsing dates and times by tweaking format strings can be mind-numbing, so it's worth considering loading [the anytime package](http://dirk.eddelbuettel.com/code/anytime.html), which will automatically recognize many more formats than `as.IDate` and `as.ITime` normally will.

#### Datetimes

Data.table also can parse a datetime string (well, a factor) into separate IDate and ITime columns:

```{r datetime-combo}
DT = data.table(dt = "2017-01-01 01:11")
DT[, c("d", "t") := IDateTime(as.factor(dt))][]
```

However, it does not include a combined datetime class, which would be useful if we want to measure time differences across days. 

For a combined datetime class, we have to use POSIXct. This brings with it a couple of annoyances. First, it uses floating-point measurement, so grouping on it with `by=` is not a good idea and some table operations will be slower. Second, in keeping with POSIX rules, it has an irritating time zone attribute that always demands careful watching. Nonetheless, it is the standard option for a datetime format currently. (There is also a POSIXlt format, but data.table discourages it, since it does not build on atomic vectors.) 

#### Sub-second measures

The ITime class is an integer measure of seconds since midnight. POSIXct will be necessary if finer-measured seconds are needed (e.g., from online activity data). If even finer measurement is needed (e.g., for network latency measurement), [the `nanotime` package](http://dirk.eddelbuettel.com/code/nanotime.html) is available.

#### Computations

There are many convenient extractor functions, `month`, `wday` and so on, listed in `?IDateTime` and `?weekdays`.

The `max` or `min` of a vector of dates can be taken (just as it can be for numbers, characters and ordered factors); and inequalities also work.

Arithmetic on IDates with `+` and `-` reads `1L` as one day; and on ITimes, as one second. 

Time differences between dates or times can be taken using `difftime`, which allows for the specification of units of measure for the output.


### Character columns

With character columns that combine multiple variables, there are a few options. 

- If the "split" of the variable is fixed-width, `substr` or `substring` works.

    ```{r read-substring}
    DT = data.table(x = c("A001ABC", "B001DEF"))
    DT[, `:=`(x_id = substring(x, 1, 1), x_num = substring(x, 2, 4), x_val = substring(x, 5, 7))][]
    str(DT)
    ```

- If the split follows a pattern that can be characterized using regular expressions (\@ref(strings)), `tstrsplit` can make the split and automatically convert to appropriate column types just as `fread` does:

    ```{r read-tstrsplit}
    DT = data.table(x = c("A_1_22.2", "B_33_44.4"))
    DT[, c("x_id", "x_int", "x_float") := tstrsplit(x, "_", type.convert = TRUE)][]
    str(DT)
    ```

With US states, built-ins (\@ref(built-ins)) and an update join (\@ref(joins-update)) can be used to convert between abbreviated and long forms:

```{r read-usstates, echo=-3}
DT = data.table(state_code = c("MN", "KS"))
DT[.(state_code = state.abb, state_name = state.name), on=.(state_code), 
   state_name := i.state_name 
]
DT
```

In the case of messier strings, the convenience functions in the stringi package may be helpful, along with a read of `?regex` and the string section in the next chapter (\@ref(strings)).

### Factor columns

`fread` will read string columns in as character by default, and this is usually the way to go.

When "recoding" values in a character column, I suggest being explicit by storing the mapping in its own table and doing an update join (\@ref(joins-update)):

```{r read-remap, echo=-7}
# example data
DT = data.table(v = c("Yes", "argle", "bargle", "No", "Yep", "garble", "Nope", "blargh"))
# desired mapping
v_translateDT = rbind(
  data.table(v_old = c("Yes", "Yep"), v_new = TRUE),
  data.table(v_old = c("No", "Nope"), v_new = FALSE)
)
# update join
DT[v_translateDT, on=.(v = v_old), vt := i.v_new ]
# ^ okay, so apparently that is printing when it shouldn't...
```

Here, we are mapping to a logical column. Of course, the same thing could be done with a factor, an ordered factor or a string.

### List columns

Data.table supports columns of class `list`. A "list column" is a list with a length equal to the number of rows of the table. Each element of the list (that is, each row) can hold anything, just like a normal list. We saw an example of this in \@ref(rbindlist-read).

List columns are rarely a good fit with tabular data for a few reasons. First, they make the data structure significantly harder to understand. Second, grouping, sorting or merging on them is not supported and probably never will be. Third, they do not play nice with time series operators like `shift` (\@ref(lag)).

They are good for many things, but should be avoided if possible while cleaning the primary data. Nonetheless, `fread` does support reading list columns in.

### `fwrite` to write delimited files {#fwrite}

To write a CSV or other delimited-table format, `fwrite` is usually the best option.

It is fast and includes nice features for printing dates and times, like the `squash` option that will print dates like `20171020`.

### Saving and loading R objects {#dt-saveload}

To save a table for later use in R, there are native formats, RData and RDS. In contrast with delimited files like CSVs, these files are small; are quick to save and load; and will allow the later user to skip tedious and error-prone data processing tasks.

Another consideration is whether the data storage is long-term or short term. If the objects are important and need to be accessible in 10-20 years, it would naturally be safer to store them in flat delimited file (assuming a tabular format makes sense for them). On the other hand, R's storage formats will always be accessible by opening a suitable old version of R; and [R's formats are open and (implicitly) documented](http://r.789695.n4.nabble.com/RData-File-Specification-td917010.html) and so should presumably be readable with a little work.

One thing to watch out for when loading data.tables from disk is broken "self" pointers, leading to an error about "Invalid .internal.selfref". This can be fixed by using `setDT` or `alloc.col` on all loaded tables, as explained in `vignette("datatable-faq")` under "Reading `data.table` from RDS or RData file". Generally, I don't bother with this safety precaution and haven't run into problems yet, so I'm not sure how critical it is.

#### Collections of objects

The `.RData` format allows saving and loading multiple objects. When loading, the objects are dropped into the main workspace by default. To avoid the namespace conflicts this invites, I usually load to a new environment:

```{r read-saveload}
# note: this code will save to your current working directory
# type getwd() and read ?setwd for details
n   = 10
L   = list(a = 1, b = 2)
dat = data.table(v = 3)
save(n, L, dat, file = "stuff.rdata")

# and later, in another script
rm(list = ls()) # clear workspace
n   = 20          # create a different n
load("stuff.rdata", env = e_stuff <- new.env())
```

Now, `n` is still 20, while the loaded value of `n = 10` can be accessed like `e_stuff$n`. The `$` and `[[` extractors for lists (\@ref(extract-list)) also work on environments, as does `names(e_stuff)`.

One nice feature of the R console is that you can drag an `.rdata` file into it to load. Also, opening an `.rdata` file with the R console will start a new session and load it. (At least this is true in Windows.)

#### Single objects

Another option is the RDS format. This stores a single R object, so there is no need to fiddle with environments:

```{r read-saveloadrds}
# note: this code will save to your current working directory
# type getwd() and read ?setwd for details
dat = data.table(v = 3)
saveRDS(dat, file = "dat.rds")

# and later, in another script
old_dat <- readRDS("dat.rds")
```

### Reading and writing other formats

For "foreign" table formats (Excel, Stata, etc), use an internet search. These input and output tools will always be in active development because the formats themselves are in flux. of course, it is unwise to use any such formats for long-term data storage.

### Formatting the display of columns

While the way R displays dates (like `2017-11-22`) might not align with what is seen elsewhere, I suggest getting used to it rather than seeking a workaround, though I'm sure there are some (like making a new class). It will be easier for everyone looking at the data if it is in a standard format.

For the display of numbers, again, I suggest leaving it alone instead of, e.g., rounding or truncating everything. If it's really an issue that must be addressed, read about `scipen` in `?options`.


## Exploring data {#exploring-data}

range, summary

Henk Harmsen on "ergonomics" https://rpubs.com/carbonmetrics/datatable_ihub

`e_bad = DT[, var, by=e_id][var==TRUE, e_id]; DT[.(e_id)] (simpler with having)`

`DT[, .N, by=e_id][order(-N)]`

creating temp vars, printing with []

DT[, do_stuff, j] %>% print(nrow=Inf) with magrittr (warning about performance hit in real nonexploratory code)

hist, hexbin

cor, cormat

x11() for multiple windows

eyeball-exploring models: curve

qqplot etc for the statistically literate, and other tools for ML people

### Browsing loaded objects {#browse-env}

ls()

rm()

tables()


### Subsetting {#dt-subset}

#### Selecting by value

This is analogous to a SQL WHERE clause or a Stata `if` clause.

```{r dt-sub-pre}
# example data
DT = data.table(Titanic)
```

```{r dt-sub}
DT[ Class == "Crew" & N > 100 ]
```

This syntax, `DT[i]`, is the subsetting task needed in the vast majority of cases. The rest of \@ref(dt-subset) can safely be skipped the first time through; it is mostly useful for reference.

#### Selecting by group statistic

This is analogous to a SQL HAVING clause.

```{r dt-sub-gstat}
DT[ , if (sum(N) < 300) .SD, by=Class ]
```

We are including the group only if it meets our condition. The structure `if (cond) x` returns `x` if the condition is true and nothing (`NULL`) otherwise, omitting those rows. `.SD` refers to the Subset of the Data associated with each `by=` group.

This syntax isn't as clean as a HAVING clause, but it is likely to improve.

#### Selecting rows within each group

```{r dt-sub-grow}
# select first row
DT[ , .SD[1L], by=Class ]
# select row(s) with highest count
DT[ , .SD[N == max(N)], by=Class ]
```

```{block2, type='rmd-details'}
**Efficient selection of rows within each group.**  The second task here is [somewhat inefficient](https://github.com/Rdatatable/data.table/issues/735) currently, with a faster workaround [provided by eddi](http://stackoverflow.com/a/16574176): `DT[DT[ , .I[N == max(N)], by=Class ]$V1]`. And [Arun provided a further improvement](http://stackoverflow.com/a/31854111), taking advantage of `gmax`, an efficient by-group `max` (see `?GForce`): `DT[DT[, max(N), by=Class], on=.(Class, N = V1)]`. The variable `.I` represents the row number in the full table, or at least it *should* do that. Currently, `.I` does [not always behave like this](https://github.com/Rdatatable/data.table/issues/1494). "V1" is the default name given to the result in `j`. 
```

#### Finding where a condition holds

To find where a condition holds:

```{r which}
cond = c("aleph", "baz", "boz") > "b"
which(cond)
```

So, it takes us from a logical index to one of positions. This can sometimes speed up computations; and is necessary when facing functions that don't take logical indices.

When using `which` on a matrix or array (see \@ref(matrix-array)), we can find out "where" in terms of row and column using the `arr.ind` option.

Beware of using `which` output as an index:

```{r bad-which}
x  = 1:5
w  = which(x^2 > 50)
x[-w]
```

We wanted to filter to where `x` meeting our condition, but instead we lost the whole vector. Straight-up logical filtering is the safer route here.

Data.table has `DT[i, which = TRUE]` syntax as well.


### Sorting

order(x, -y)

setorder

setkey

### Summaries {#dt-summary-stats}

draw from http://stackoverflow.com/documentation/data.table/3785 


#### The `summary` function

similar to stata

doesn't really return a useful object, only for printing

#### Collapsing a table

collapse

various custom summary functions

maybe mention GForce again


#### Counting missing values

mdesc, count missing per column colSums(is.na(.SD))


#### Tabulating {#counting-rows}

Tabulating or tallying 

bwDT[, .N, by=ftv]

refer to table() and plot.table()

With a continuous variable `x`, it often doesn't make sense to tally values as in the last section. However, we can specify bins and find where the values fall:

```{r cut}
set.seed(1)
n  = 1000
x  = rchisq(n, df = 10)
xc = cut(x, breaks = c(0, 5, 10, 15, 20, Inf)) # or max(x) in place of Inf
table(xc)
```

See the note in `?cut` for details on more efficient alternatives:

> Instead of `table(cut(x, br))`, `hist(x, br, plot = FALSE)` is more efficient and less memory hungry. Instead of `cut(*, labels = FALSE)`, `findInterval()` is more efficient.



### Reshaping to wide {#dcast}

draw from http://stackoverflow.com/documentation/data.table/4117

wide format is rarely useful for storing data (see structuring data)

but it is good for browsing

dcast

analogy with blacksmithing or something (re cast and melt), molten into its most flexible form, cast into a rigid shape for a particular purpose


todo cite `vignette("datatable-reshape")`



## Organizing relational tables {#structuring-data}

background: tidy data / database design

data.table(C02) might be a good example, with its redundant cols

### Reshaping to long format

todo cite `vignette("datatable-reshape")` again


### Choosing tables and keys


### Listing levels

make a "skeleton" for the data

consider igraph if the order does not matter

use CJ(col1, col2), maybe seq

if it's not cartesian, then DT[, col2, by=col1]


### Exploring levels

setdiff, union, intersect (works with data.table from 1.9.8+)

does the table have any dupes?

does the table have all levels?

stopifnot

### Collapsing {#aggregate-join}

essentially an aggregate join -- we join so that we can aggregate

maybe collapsing because we want a table aggregated at a higher level

maybe collapsing because we have "duplicates" hanging around for other reasons

unique, duplicated + fromLast

```{block2 dt-join-beware, type='rmd-caution'}
**Beware `DT[i,on=,j,by=bycols]`.** Only `by=.EACHI` works in a join. Typing other `by=` values there will cause `i`'s columns to become unavailable. This [may eventually change](https://github.com/Rdatatable/data.table/issues/733).
```

use CJ for missing levels

### Expanding

CJ + unique


use CJ for missing levels

Suppose we have a table measuring wealth by person and year.

```{r expandrows-pre, echo=1}
DT = data.table(psn_id = c(1L, 1L, 2L), year = c(1998L, 2000L, 2000L), wealth = 24:22)
DT
```

Now we want a person-year row for every every person and for every year from the lowest to the highest. We can do this with `CJ` and a standard join:

```{r expandrows, echo=1}
DT = data.table(psn_id = c(1L, 1L, 2L), year = c(1998L, 2000L, 2000L), wealth = 24:22)
DT
```


### Update joins

we join so that we can update the table on the left


### Programming data.table calls

getting into the weeds

DT[, if (.GRP == 1) {}, by=.(x,y,z)] for testing, or alternately browser()

eval-quote for constructing efficient calls

mget / get


review related options

options() %>% .[grep("datatable", names(.))]

#### Specifying columns {#program-cols}

shortcuts when entering j, by, .SDcols

a:b, .(a,b), !(whatever), -(whatever)

for .SDcols -- logical or integer instead of character (though I can't see any reason to use those)


shortcuts: .SDcols = V1:V10, .SDcols = grep("^date_", names(DT)), etc


```{r lapply-dt, eval=FALSE}
DT[, 
   lapply(.SD, max)
, by=z, .SDcols = { sapply(DT, is.numeric) }]
```

`.SDcols` is used to filter the columns that appear in `.SD`, the subset of data available in `j`. 

### Bad aggregations

todo -- i prolly don't need a section; there are countless ways to format data badly. but i do want to warn about by=1:nrow(DT) et al... for now, i'll put a warning block in the agg section higher up

Suppose we have data like ...

```{r dt-badagg-pre, echo=1}
badDT = data.table(col = c("A","A","B"), A = 1:3, B = -1:1)
badDT
```

Values in the `col` column refers to the other columns. We might want to do things like ...

```{r dt-badagg, eval=FALSE}
# select the column using col
badDT[, .SD[[col]], by=1:nrow(badDT)]
# take the larger column
badDT[, max(unlist(.SD)), by=1:nrow(badDT), .SDcols=c("A","B")]
```

Or we might have data like ...

```{r dt-badagg2-pre, echo=1}
badparDT = data.table(parent = c("A1","A2","B1"), kid1 = c("a","a","b"), kid2 = c("d", "d", NA), kid3 = c("f", "f", NA))
badparDT
```

Parent names consist of the family code plus a number; and kids are arrayed on columns, with as many such columns as are needed for the largest family. We might want to do things like ...

```{r dt-badagg2, eval=FALSE}
# count kids per parent
badDT[, .SD[[col]], by=1:nrow(badDT)]
# count parents per family
badDT[, max(unlist(.SD)), by=1:nrow(badDT), .SDcols=c("A","B")]
```


While such code might technically run, 

... split this section into iterating over columns and iterating over groups. explain that by= will take any appropriate-length vector, warn about by=1:nrow(DT)





