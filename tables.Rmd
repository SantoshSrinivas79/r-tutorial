-- todo prolly use fewer distinct example data sets. this can be done after everything's in and organized throughout the full tutorial --

# Tables {#tables}

This chapter covers navigating tabular data with R. The core syntax is (\@ref(dt-syntax)):

    DT[where, select|update|do, by] # SQL verbs
    DT[i, j, by]                    # R function arguments

It reads as:

1. Subset using `i`, then
1. Group using `by`, then
1. Do `j`


## Essential packages

This section covers packages that help me work more efficiently, to the point where I regard them as essential. They'll be used throughout the rest of this document. They don't have any dependencies on other packages, and I expect they'll be available as long as R is. It will be assumed in subsequent sections that these libraries have been attached:

```{r loadem, warning=FALSE}
library(magrittr)
library(data.table)
```

### The magrittr package {#magrittr}

[Magrittr](https://cran.r-project.org/package=magrittr) introduces syntax with "pipes," improving readability by unnesting function calls:

```{r magrittr-calls, eval=FALSE}
# we can do
x %>% f %>% g %>% h
# instead of 
h(g(f(x)))
```

In addition, it allows more compact function definitions (\@ref(function-writing)):

```{r magrittr-funs}
fun = . %>% .^2 %>% sum %>% sqrt
fun(1:3)
```

Despite these advantages, magrittr can slow down code, so avoid using it anywhere where speed might be an issue.

To install, just use CRAN:

```{r inst-mgr, eval=FALSE}
install.packages('magrittr')
```

The name is a pun on [RenÃ© Magritte's pipe painting](https://en.wikipedia.org/wiki/The_Treachery_of_Images). To get a handle on the package, I would start with the package [vignette](https://CRAN.R-project.org/package=magrittr/vignettes/magrittr.html).

### The data.table package {#data-table}

[Data.table](http://r-datatable.com) offers a variant of the `data.frame` class for data frames (seen in \@ref(data-frames)), optimized for fast sorted and grouped operations and enhanced with cleaner syntax. Section \@ref(dt-class) introduces this `data.table` class. 

The package also bundles in a variety of other functionality:

- ITime and IDate date and time classes
- `fread` and `fwrite` for fast reading and writing of delimited files
- `dcast` and `melt` for reshaping tables

Because these disparate features are bundled together in data.table, we don't have to load more packages and worry about their complicated dependencies or namespace conflicts (\@ref(namespaces)).

#### Installation

Again, CRAN can be used:

```{r inst-dt, eval=FALSE}
install.packages('data.table')
```

However, the package is under active development, with new features available only in the development version. To install it, follow [the instructions from the package wiki](https://github.com/Rdatatable/data.table/wiki/Installation):

```{r inst-dt-dev, eval=FALSE}
remove.packages("data.table")
install.packages("data.table", type = "source",
  repos = "http://Rdatatable.github.io/data.table")
```

If using Windows, you'll need to first install [Rtools](https://cran.r-project.org/bin/windows/Rtools/) (requiring administrator privileges), as explained in the last link.

#### Getting started

The official vignettes for the package are a great way to start with the package. See them on the [wiki](http://r-datatable.com/Getting-started) or with 

```{r, eval=FALSE}
browseVignettes(package="data.table")
```

(Beware that as of March 2017, the website vignettes are somewhat out-of-date.) The website also includes links to other useful materials (an online course, presentation materials, blog posts). Before using the package, I started by reading some slides and the FAQ in full.



## The data.table class {#dt-class}

Data.tables extend the data frame class introduced in \@ref(data-frames). 

This section reviews basic operations seen in the last chapter (inspecting, slicing, extracting), before discussing how `data.table` extends `data.frame`.

### Inspecting {#dt-inspect}

Consider the `quakes` data set (with more info in `?quakes`):

```{r dt-view-pre, echo=1}
quakeDT = data.table(quakes)
quakeDT
```

By default, data.tables with over 100 rows will print in the compressed form seen above, showing just the first and last five rows. To globally change how many rows are needed, use `options(datatable.print.nrows = n)`. To override it a single time, use `print` with `nrows=`. I usually set `nrows=Inf` to print all rows:

```{r dt-print, eval=FALSE}
quakeDT %>% print(nrow = Inf)
```

To browse a table in a new window, `View` for data frames (\@ref(view)) again works:

```{r dt-view, eval=FALSE}
quakeDT %>% View
# or if using RStudio
quakeDT %>% utils::View()
```

To inspect the structure of a data.table, we can again use `str` (\@ref(str)):

```{r dt-str}
quakeDT %>% str
```

Here, we see the class of each column along with the first few values. In addition, there is an obscure `".internal.selfref"` attribute, which we can ignore except when saving or loading the table from disk (\@ref(dt-saveload)). Other attributes will sometimes also show up here, related to optimizing the performance of ordered or grouped queries on the table (\@ref(keys-indices)).

In the code above, we could use syntax like `View(quakeDT)` instead of `quakeDT %>% View`, but I often find the latter handier, since it's easy to insert intermediate steps, like `quakeDT %>% head(10) %>% View`. Also, while `%>%` is slow, it's not going to matter for tasks like browsing data.

### Slicing {#dt-slicing}

A slice of a data.table is a smaller data.table formed by subsetting rows, columns, or both, analogous to vector and matrix slices seen in \@ref(slicing) and \@ref(slicing-matrix). The remaining columns will retain attributes from the full table (class, levels for factors, etc.).

The syntax is familiar from vectors and matrices:

```{r dt-slicing-pre}
# example data
DT = data.table(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
```
```{r dt-slicing}
DT[1:2, ]
DT[, "z"]
DT[-3, c("y", "z")]
```

One difference with matrices is that when only slicing rows, we can skip the comma:

```{r dt-slicing-rows}
DT[1:2]
```

Other differences come into play when subsetting columns programmatically, where we need either a `..` prefix...

```{r dt-slicing-cols-vec, error=TRUE}
keep_cols = c("y", "z")
DT[, keep_cols]                  # error
DT[, ..keep_cols]                # use this instead
```

... or `with=FALSE` if programming inline:

```{r dt-slicing-cols-program}
DT[, letters[25:26]]             # no error, but doesn't print what we want
DT[, letters[25:26], with=FALSE] # use this instead
```

These requirements are a quirk of the more flexible `DT[...]` syntax that supports far more than taking slices, as discussed in \@ref(dt-syntax).

```{block2 dt-colnumbers, type='rmd-caution'}
**Use column names, not numbers.** Subsetting by hard-coded column number, like `DT[, 2:3]` or `cols = 2:3; DT[, ..cols]` works but is discouraged. Column numbers can easily change in the course of writing or updating a script, invalidating column number references in a way that will be annoying to debug. See the first answer in `vignette("datatable-faq")` for a deeper discussion.
```

```{block2 dt-dfbrackafrak, type='rmd-details'}
**How data frame slicing works.** This is really getting in the weeds, since I suggest not using data frames at all, but you'll see two major differences if you do. First, `DT[, "z"]` will extract the `z` column instead of taking a slice thanks to data frames' `drop=TRUE` default (which also came up regarding matrices in the last chapter). Second, `DT[1:2]` will slice the first two *columns* instead of the first two rows, thanks to the fact that data frames (and data.tables) are lists and the no-comma usage of `[` triggers list slicing.
```

`head`, `tail` and the special empty and missing-data slices (all seen in the last chapter) work by row:

```{r dt-slice-headtailspecial}
head(DT, -3)    # top, removing last 3
tail(DT, 2)     # bottom 2
DT[0L]          # empty 
DT[NA_integer_] # missing
```

The package also offers `first` and `last`, which are simply special cases of `head` and `tail`.

Fancier slicing methods, like `DT[x > "b", .(y, z)]`, will be introduced with the rest of the `DT[...]` syntax in \@ref(dt-syntax).

There are some syntactical shortcuts for slicing to a set of columns in `j`; see \@ref(program-cols).

### Extracting columns

Since data.tables (and data frames) are lists, we can extract columns like `DT$z` or `DT[["z"]]`. 

With the full `DT[...]` syntax (\@ref(dt-syntax)), it is easy to extract a column for a limited set of rows, like `DT[x > "b", z]`.

### Extensions to the data.frame class

The first thing to note is that data.tables *are* data frames, so any function that works on a data frame will work fine with data.tables, too (even if the data.table package is not installed or attached).

#### Modification in-place

In contrast with the rest of R, data.tables are primarily modified "in-place" (or "by reference"), which can be much more efficient. As a first example, we can switch the class of a data frame to data.table:

```{r setDT-pre}
# example data
DF = data.frame(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
```
```{r setDT}
str(DF)
setDT(DF)
str(DF)
```

We did not use `=` or `<-` to assign the result of `setDT`; the function simply altered `DF` in-place. All of data.table's functions named like `set*` do this.

```{block2 dt-byref, type='rmd-details'}
**R's copy-on-modify.** Understanding the contrast between modification in-place and base R's modification rules (often called "copy-on-modify") probably requires some familiarity with C (see threads on [stackoverflow](http://stackoverflow.com/questions/15759117/what-exactly-is-copy-on-modify-semantics-in-r-and-where-is-the-canonical-source) or [the mailing list](http://r.789695.n4.nabble.com/Confused-about-NAMED-td4103326.html) if interested). I wouldn't worry about it except to note that many base R operations make copies, which is costly in terms of RAM and computing time; while those operations' data.table counterparts do not have this problem.
```

#### Factor vs character columns {#dt-factorvchar}

Another difference is that `data.frame(...)` reads string input as a `factor` categorical variable (\@ref(factors)), as seen in the `str` output above; while `data.table(...)` reads it as character:

```{r DT}
DT = data.table(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
str(DT)
```

Data frames' preference for factors may still bite when reading files in with `read.table` from base R instead of `fread` from data.table (\@ref(fread)).

#### Extended syntax for `DT[...]`\ {#dt-syntax}

The third major difference is the extension of the `DT[...]` syntax to support more than simple slices (like `DF[i,j]`, where `i` and `j` as indices, covered in \@ref(dt-slicing)). It offers SQL-style syntax that is cleaner, particularly for by-group operations:

    # (pseudocode)
    DT[where, select|update|do, by] # SQL verbs
    DT[i, j, by]                    # R function arguments

This should be read as a command to take a sequence of steps:

1. Subset using `i`
1. Group using `by`
1. Do `j`

We can use column names as barewords, and even form expressions in terms of columns:

```{r DT-ex}
DT[x > "b", sum(y), by=z]
```

Typically, the `i` and `j` arguments are called by position, as seen here; while `by=` is called by name. 

Whenever `j` evaluates to a list, the output will be a new data.table:

```{r DT-exdot}
DT[x > "b", .(n_bigy = sum(y > 2), n_smally = sum(y <= 2)), by=z]
```

```{block2 dt-dot, type='rmd-caution'}
**The `.()` convenience function.** Inside many arguments of `DT[...]`, we can use the shorthand `.()`, which stands for `list()`.
```

The use of `by=` is very similar to a `for` loop (\@ref(for-loops)) over subsets, but it is better in a few important ways: 

- We don't have to manually construct and keep track of some "split-up data" list to iterate over.
- Fast by-group functions are used when available (see `?GForce`)
- We don't have to define the task in `j` as a function of prespecified variables -- we can just use any columns of the data.table.
- We don't have to worry about intermediate variables (like `case` in the next example) contaminating the global environment.

There are some syntactical shortcuts for writing a list of columns in `by=`; see \@ref(program-cols).

#### An example

The task in `j` can really be anything. Just as a demonstration of its power, here's how it can be used for saving per-group plots to a PDF:

```{r dt-bydemo, cache=TRUE}
# note: this code will save to your current working directory
# type getwd() and read ?setwd for details

bwDT = data.table(MASS::birthwt)

pdf(file="birthweight_graphs.pdf")
bwDT[, {
  case = sprintf("high #visits? = %s, smoking? = %s", .BY$high_vis, .BY$smoke)
  
  cat("Handling", case, "...\n")
  plot(
    age ~ lwt, 
    main = case
  )  
}, by=.(high_vis = ftv >= 1, smoke)]
dev.off()
```

Plotting graphs is beyond the scope of this document, but the example above should make intuitive sense after reading the docs for each object, `?MASS::birthwt`, `?pdf`, et al. For each group, we're formatting a string with `sprintf`; printing it with `cat`; and saving a plot. The special symbol `.BY` is a list containing the per-group values of the `by=` variables. Formatting and printing strings will be covered more in \@ref(strings).

```{block2 dt-logicolsub, type='rmd-caution'}
**Filtering on logical columns.** `DT[z]` and `DT[!z]` will give errors, for design reasons related to the syntax for joins (\@ref(dt-joins)). To get around this, always wrap the column in parentheses: `DT[(z)]` and `DT[!(z)]`.
```

```{block2 dt-byfloats, type='rmd-caution'}
**Grouping on floats.** Much of the computational and syntactical magic of the package comes from grouping rows together with the `by=` argument. To group on a floating-point variable, however, is just asking for trouble, [for the usual numerical computing reasons](http://floating-point-gui.de/). Instead, always use characters, integers or factors. To discretise a continuous variable into bins, use `cut`.
```

```{block2 dt-verbose, type='rmd-details'}
**Verbose data.table messages.** To learn how data.table queries work, I recommend toggling the setting `options(datatable.verbose = TRUE)`. This option is similar to verbose output from an optimization call (reporting the value of the objective at each iteration, etc.). To only see verbose output for a single call, add `verbose = TRUE`. For example,`DT[x > "b", sum(y), by=z, verbose=TRUE]`.
```


## Aggregation

The mtcars data set (see `?mtcars`) has two categorical variables: 

- `am` for automatic (0) or manual (1) transmission; and 
- `vs` for v (0) or straight (1) engine shape. 

```{r agg-pre}
# example data
carsDT = data.table(mtcars, keep.rownames = TRUE)
# quick inspection
first(carsDT)
```

Suppose we want to compare the mean horsepower, `hp`, across these categories:

```{r agg}
carsDT[, .(mean_hp = mean(hp)), by=.(am, vs)]
```

So, we just write `j` of `DT[i,j,by]` as an expression to compute the summary statistic, optionally giving it a name by wrapping in `.(name = expression)`.

Section \@ref(dt-summary-stats) covers more options for exploring data with summary statistics; and \@ref(dcast) shows how to put this result in wide format (with, e.g., `am` on rows and `vs` on columns).

### Iterating over columns {#dt-lapply}

Now suppose we want to compare mean horsepower, weight and displacement:

```{r agg-sd, eval=-(1:2)}
carsDT[, .(mean_hp = mean(hp), mean_wt = mean(wt), mean_disp = mean(disp)), by=.(am, vs)]
# can be simplified to...
carsDT[, lapply(.SD, mean), by=.(am, vs), .SDcols = c("hp", "wt", "disp")]
```

So, we just write the relevant columns in `.SDcols` and refer to `.SD`, the ***S**ubset of **D**ata*. Within each `by=` group, the query has access to `.SD` with rows for that group and columns as specified in `.SDcols`. `.SD`, like `.BY` seen earlier, is a special symbol available in some arguments of `DT[...]`, documented at `?.SD`.

We can use `lapply` (a function designed for iterating over lists) here since `.SD` is a data.table, which is a list of column vectors (see \@ref(lapply-df)). The column names carry over to the result because `lapply` always carries over names. 

While dot notation `.SDcols=.(hp, wt, disp)` is not yet supported, there are a variety of convenience features for specifying `.SDcols`, covered in \@ref(program-cols).

```{block2 dt-avoidbyrow, type='rmd-caution'}
**"Aggregating" across columns.** One major red flag to look out for is the desire to "aggregate" columns by row, setting `by=1:nrow(DT)` and possibly using `unlist(.SD)` somewhere. Not only will this be incredibly slow, but it also suggests that the data is poorly organized, costing a lot of extra mental energy at every step. Section \@ref(structuring-data) explains some ways to format data better to avoid the need for this problematic approach.
```


## Modifying data {#dt-subassign}

Creating, editing and removing columns are all done using `:=` in `j` of `DT[i, j, by]`. This functionality operates in-place, in the sense that the underlying data stays in the same place, which is more efficient in terms of how much RAM and time is taken. See `vignette("datatable-reference-semantics")` for details.

```{block2, type='rmd-details'}
**What can be modified in-place?** The scope of in-place operations is currently limited to altering columns. Adding and removing rows in-place is not yet supported; it is harder to do in R, due to its column-oriented storage of tables (contrasting with database systems that store data rowwise for easy INSERT and DELETE queries).
```

### Creating columns {#dt-col-create}

```{r dt-add-pre, echo=1:2}
# example data
DT = data.table(
  x = letters[c(1, 2, 3, 4, 5)], 
  y = c(1, 2, 3, 4, 5), 
  z = c(1, 2, 3, 4, 5) > 3
)
DT
```
```{r dt-add}
# creating a column
DT[, u := 5:1][]
# creating multiple
DT[, `:=`(v = 2, w = 3L)][]
# creating with dynamic names
nms = c("a", "b", "c")
DT[, (nms) := .(1, 2, 3)][]
```

All of these tasks are performed in-place -- altering `DT` without making a new object. Usually, the results are not printed in the console; but they appear here because `[]` is "chained" onto the end of each task.

```{block2 modify-op, type='rmd-caution'}
**`:=` is the function *creation, modification and deletion* of columns.** This will be covered it more detail in subsequent sections, but is worth emphasising. In particular, this contrasts with Stata (which uses distinct verbs `gen`, `replace` and `drop`).
```

```{block2 modify-iter, type='rmd-details'}
**Iterative column creation.** The `` `:=`(...)`` syntax does *not* support iterative definitions like ``DT[, `:=`(W1 = u + y, W2 = W1^2)]``. One common workaround is ``DT[, `:=`(W1 = W1 <- u + y, W2 = W1^2)]``. This solution may not be intuitive for new R users, but the gist is: `W1 <- u + y` creates `W1` as an object in `DT[...]` and then returns its value, let's call it `v`. Now, `W2 = W1^2` can find `W1`, since it was created by `<-`; and `W1 = W1 <- u + y` simplifies to `W1 <- v`, where `v` is the return value of `W1 <- u + y`.
```

### Removing columns

```{r dt-remove}
nms = c("u", "v", "w", "a", "b", "c")
DT[, (nms) := NULL][]
```

A warning will print if we remove some columns that don't currently exist.

### Replacing entire columns

Data.table is careful about column types:

```{r dt-reptot, warning=TRUE}
DT[, a := 10L ][]  # Create a new column of integers
DT[, a := 21L ][]  # Replace it with another column of integers
DT[, a := 32.5 ][] # Replace it with a float
```

The warning in the last call is related to coercion of vector classes (\@ref(classes)).

```{block2 modify-typesafe, type='rmd-details'}
**Safeguards against accidental coercion.** The verbose warning printed above is typical for the package and an excellent feature. Running the final command, data.table knows that the `a` column is of integer type and sees that 32.5 is conspiciously *not an integer*. So it gives a warning when coercing 32.5 to an integer (to match `a`). 

Elsewhere in R, `a` would be coerced to match the float `32.5` -- probably not the behavior we want -- with no warning. That is, if we have `x <- c(21L, 22L)`, we can freely assign `x[1] <- 32.5`; and the same freedom (by which I mean "danger") is present even if `x` is a data frame column. For more on the issue, search online for "type safety."
```

If we want this assignment to work, we need to change `a`'s type by passing a full vector, as described in the warning:

```{r dt-reptot-class, warning=TRUE}
DT[, a := rep(32.5, .N) ][]
```

The coercion is done silently, but it can be made more visible by turning on `verbose`, which notes the "plonk" of a full vector replacement.

### Replacing columns conditionally {#dt-ifelse}

Conditional replacement here is analogous to a SQL UPDATE query or a replace if command in Stata. To illustrate, we will look again at a vectorized `if`/`else` assignment, mentioned in \@ref(ifelse):

```{r dt-ifelse-1, results="hide"}
DT[     , b := "Aardvark"] # initialize to baseline value
```
```{r dt-ifelse-2}
DT[y > 1, b := "Zebra"][] # replace based on a condition
```

This can also be done with chaining:

```{r dt-ifelse-chain, results="hide"}
DT[, b := "Aardvark"][y > 1, b := "Zebra"]
```

This chaining works because `DT[...][...]` is evaluated like `(DT[...])[...]` and the return value of the piece in parentheses is `DT` -- provided `j` has a `:=` statement.

```{block2 modify-chainwarn, type='rmd-caution'}
**Broken chains.** If `j` is not a `:=` statement, the return value is not the original data.table but rather a new one. Subsequent steps in the chain will not affect the starting table. So, after `DT[y < Inf, d := 1]` and `DT[y < Inf][, d := 2]`, what does the `d` column look like in `DT`? See the Exercise section of `vignette("datatable-reference-semantics")`; and the Note section of `` ?`:=` ``.
```

As we saw in the last section, partial replacement of a column will trigger a warning if the classes don't match:

```{r dt-ifelse-conflict, warning=TRUE, results="hide"}
DT[, b := "Aardvark"][y > 1, b := 111]
```

Another nice feature, similar to Stata, is reporting of the number of rows modified. This can be seen by turning `verbose` on:

```{r dt-ifelse-count}
DT[, b := "Aardvark"][y > 1, b := "Zebra", verbose = TRUE]
```

### Other in-place modifications

The data.table package has a few other tools for modifying table attributes in-place:

- The `set` function is another way of making assignments like `:=`.

- `setDT` and `setDF`, seen earlier, alter the class.

- `setorder` will sort the table by some or all of its columns.

- `setcolorder` changes the order in which columns are displayed.

- Indices and the key (explained in \@ref(keys-indices)) can be set with `setindex` and `setkey`, respectively.

- `setnames` will alter column names. For example, in \@ref(dt-lapply) we saw...

    ```{r modify-namesexpost-pre}
    carsDT[, lapply(.SD, mean), by=.(am, vs), .SDcols = c("hp", "wt", "disp")]
    ```

    ... and if we want to add the prefix `"mean_"` to the results, we can do

    ```{r modify-namesexpost}
    cols = c("hp", "wt", "disp")
    carsDT[, lapply(.SD, mean), by=.(am, vs), .SDcols = cols] %>% 
      setnames(cols, sprintf("mean_%s", cols)) %>% print
    ```
    
    The trailing `%>% print` command is used because, `setnames`, like `:=` and the other `set*` operators, does not print the table on its own. The string-formatter `sprintf` will be explained in \@ref(strings).


- `setattr` is a general function for altering attributes of a vector or other object (see `?attributes`). For example, if we have a factor column (encoding categorical data), we can change its "levels":

    ```{r dt-setlevels-pre}
    # example data
    fDT = data.table(fac = factor(c("A", "B"), levels = c("A", "B", "C")))
    ```
    ```{r dt-setlevels}
    levels(fDT$fac)
    fDT$fac %>% setattr("levels", c("A", "B", "Q"))
    levels(fDT$fac)
    ```


### Avoiding in-place modification

To create a new data.table starting from an existing table, use

```{r dt-copy, eval=FALSE}
DT2 = copy(DT1)
```

We have to use this instead of `DT2 = DT1` since with the latter we have only created a new "pointer" to the first table. I routinely make copies like this after reading in data (\@ref(fread)) so that I can back out where I tripped over something in the process of data cleaning.

Besides `DT2 = DT1`, `names1 = names(DT1)` is also unsafe, since the `names` function does not extract the column names as they are at a given time, but rather points at the names attribute, which can change as columns are modified or rearranged.

### Using in-place modification in functions

A user-written function (\@ref(function-writing)) like

```{r modify-fun}
f <- function(DT) DT[, newcol := 1111 ]
```

will act like `:=` and the `set*` functions, altering its input in-place. This can be useful, but requires extra caution. See the "`:=` for its side effect" section of `vignette("datatable-reference-semantics")` for discussion.


```{r todo-modify-ex, echo=FALSE, eval=FALSE}
### Exercises

use carsDT = data.table(mtcars, rn = TRUE), overwrite am with factor levels "automatic" and "manual"

```

## Joins {#dt-joins}

They are called "joins" or "merges."

```{r dt-join-prea, echo=c(1:2,4L)}
# example data
a = data.table(id = c(1L, 1L, 2L, 3L, NA_integer_), t = c(1L, 2L, 1L, 2L, NA_integer_), x = 11:15)
a
b = data.table(id = 1:2, y = -(1:2))
b
```

The idiom for a simple join is `x[i]` or `x[i, on=.(...)]`: 

```{r dt-join}
a[b, on=.(id)]
```

Think of `x[i]` as using index `i` to look up rows of `x`, in the same way an "index matrix" can look up elements of a matrix (\@ref(matrix-extract)). 

todo explain on=.(xcol = icol)



todo -- details re efficiency, citing "binary search vs vector scans" section of `vignette("datatable-keys-fast-subset")` and perhaps a benchmark

### Aggregating in a join

aggregate joins (by=.EACHI)


### Updating in a join

If the data is well organized, this will be the join needed most of the time. It modifies one table, `x`, by

update joins, motivated by relational data


extension of SQL UPDATE

### Self join to fill in missing levels

Suppose we have data per year for each ID:

```{r dt-join-CJ-pre}
DT = data.table(id = c(1L, 1L, 2L), year = c(1999L, 2000L, 1999L), v = c(5, 7, 9))
```

Now we want to complete the data set so that every ID has a row for every year:

```{r dt-join-CJ}
DT[CJ(id = id, year = year, unique=TRUE), on=.(id, year)]
```

This is called a self join because we are using the table's own columns in `i` of `DT[i, on=]`. The CJ function is a helper that takes all combinations of vectors, alternately called the Cartesian product or a "cross join."

### Handling matches

#### Handling multiply-matched rows {#join-multimatch}

In this example, we are indexing by rows of `b` and so get matches for every row of `b`. By default, we get *all* matches in `a`, but this can be tweaked:

```{r dt-join-mult}
a[b, on=.(id), mult="first"]
```

Now each row of `b` only returns the first matching row (from the top) in `a`.

#### Handling unmatched rows

Flipping it around, if we use `a` to index `b`, we have some index rows from `a` that don't have any matches in `b`:

```{r dt-join-nomatch}
b[a, on=.(id)]
```

These unmatched rows still show up in the result, which is usually nice. However, this behavior can also be tweaked:

```{r dt-join-nomatch0}
b[a, on=.(id), nomatch=0]
```



```{block2 dt-merge-res, type='rmd-details'}
**Diagnostics for merges.** In Stata, joins report on how well they went -- did everything match? how many didn't match? The analogous question for an `x[i]` join is -- for each row of `i`, how many matches did we find in `x`? To see the answer, use `b[a, on=.(id), .N, by=.EACHI]`.
```

#### Handling imperfect matches with rolling joins

Sometimes we want unmatched rows paired with the closest match occurring earlier or later in the table:

todo -- eval this after standardizing DT in the section

```{r im-rolling-im-rolling, eval=FALSE}
DT[.("F", 1:3), on=.(ID, t), roll = -Inf, .(v, .I), by=.EACHI]
```

In this way each row of `i` in `x[i]` is matched with exactly one row of `x`. 

Rolling only affects the final column in `on=`. 

The value of `roll=` refers to how much higher or lower the value of `t` can be and still qualify as a match. We add (up to) `roll` to the target row if necessary to find a match. So `roll = -7` means we would accept a `T` as far away as `T - 7 = t`. 

It can also be set to `roll="nearest"` to find the closest match in either direction (with infinite range).

### Interval joins

It is sometimes useful to match on a range of values. To do this, we explicitly name all columns in `i` and define inequalities in `on=`. Suppose we want to see, for every time `t` in 1..5, how many individuals were seen in the preceding three days:

```{r join-interval, echo=-2}
DT = data.table(
    ID = c("D", "F", "F", "B", "C", "F", "A"), 
    t  = c(1L, 1L, 9L, 11L, 13L, 13L, 16L),
    v  = c(1, 0.2, 1.2, 0.4, 1.4, 0.8, 0.6)
)
DT
tg = 1:5 # times of interest
DT[.(t0 = tg - 3, t1 = tg), on=.(t >= t0, t < t1),
   uniqueN(ID, na.rm = TRUE)
, by=.EACHI]
```

At first, these results may look unintuitive. The syntax here is again `x[i, on=, j, by=.EACHI]`. We are using `i` to subset `x`; then grouping by each row of `i` to compute `j`. The results make sense in this context. We are using `i` to find rows of `x`, so we have values from `i` and column names from `x`. Unfortunately, this means we have repeating column names. I often add a trailing `setnames` call to address this:

```{r join-interval-names}
DT[.(t0 = tg - 3, t1 = tg), on=.(t > t0, t < t1),
   uniqueN(ID, na.rm = TRUE)
, by=.EACHI][, setnames(.SD, c("t0", "t", "n"))]
```

Interval joins are also called "non-equi joins." For more complicated interval joins, have a look at `?inrange`, `?foverlaps` and the [IRanges package](http://www.bioconductor.org/packages/IRanges/) that inspired these data.table tools.

### Shortcuts and tricks

When joining on a single character or factor column, the `.()` in `i` can be skipped:

```
DT["F", on=.(ID)]
```

#### Setting keys and indices {#keys-indices}

If a table is always joined on the same column(s), these can be set as its "key." Setting the key of `x` sorts the table and allows for skipping `on=` during `x[i,on=]` joins. It also can have some performance benefits. See `?setkey`. 

beware DT[DT2] when trying to join on part of the key

```
setkey(DT, ID, t)
DT[.("F", 13)]
```

review Remarks here http://stackoverflow.com/documentation/data.table/4977

details -- Note that keys are costly, as explained in `vignette("datatable-secondary-indices-and-auto-indexing")`. Tutorials written before `on=` was available might leave the impression that keys are of central importance. Also https://github.com/Rdatatable/data.table/issues/1232#issuecomment-131190268 




#### Anti-joins

We also have the option of selecting *unmatched* rows:

```{r dt-join-notjoin}
a[!b, on=.(id)]
```

This "not-join" or "anti-join" returns all rows of `a` that are not matched by rows in `b`.

using indices

see the vignette

## Input and output {#input-output}

### File paths

Don't provide paths with backslashes, like `"C:\data\input.csv"`, since `\` is special character in R. My workaround is to use forward slashes or construct the path using `file.path`.

- For a list of files and folders, use `dir` or `list.files`. 
- For relative paths, `"."` is the current folder; `".."` navigates one level up; and `normalizePath` converts to an absolute path. 
- For the current path, use `getwd`; and to alter it, `setwd(new_path)`.
- To extract parts of a path, use, e.g., `file_ext` from the tools package (included in the base R installation).

For functions to manipulate files, see `?files` and `?dir.create`; and for file attributes, `?file.info`.

### `fread` to read delimited files {#fread}

To read tables from CSVs or other delimited formats, `fread` is quite fast and reliable:

```{r fread, eval=FALSE}
DT = fread("file.csv")
```

todo cite https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread and ?fread for a fuller list of features in lieu of a vignette

### `rbindlist` to combine tables {#rbindlist-read}

To read and combine several tables with the same columns, `rbindlist` is the right tool. The short approach is:

```{r fread-list, eval=FALSE}
rbindlist(lapply(list.files(patt="csv$"), fread), id=TRUE)
# or...
list.files(patt="csv$") %>% lapply(fread) %>% rbindlist(id=TRUE)
```

The rest of this section covers the longer approach I recommend, using a table with one row per file:

```{r fread-list-long}
# note: this code will save to your current working directory
# type getwd() and read ?setwd for details

# example data
set.seed(1)
for (i in 1:3) 
  fwrite(data.table(id = 1:2, v = sample(letters, 2)), file = sprintf("file201%s.csv", i))

# First, identify the files wanted:
fileDT = data.table(fn = list.files(pattern="csv$"))

# Next, optionally parse the names for metadata using regex:
fileDT[, year := type.convert(sub(".*([0-9]{4}).*", "\\1", fn))]

# Finally construct a string file-ID column:
fileDT[, id := as.character(.I)][]
```

From here, read in the files as a new column:

```{r fread-list-long2}
fileDT[, contents := .(lapply(fn, fread))][]
```

Then, combine them for the final table:

```{r fread-list-long3}
DT = fileDT[, rbindlist(setNames(contents, id), idcol="file_id")]

# Perhaps add in metadata using an update join
DT[fileDT, on=.(file_id = id), year := i.year ][]
```

Results can be debugged by investigating individual files like `fileDT[year == 2012, contents[[1]]]`. 

### Date and time columns {#dates-times}

Currently, `fread` does not recognize or translate time or date columns to native R formats

as.IDate, IDateTime

wday, weekdays, etc.

show that max, min, inequalities work for dates and times

lapply + .SDcols with date conversion


### Character columns

state.abb, state.names

tstrsplit

substring

### Factor columns

use characters most of the time, don't worry about memory. the real memory problems come from how data is organized, as discussed in \@ref(structuring-data).

show how ordered factors can be used to set up inequalities for strings


### List columns

list columns -- somewhat clumsy, but sometimes convenient (no by=, shift(); inefficient %in% and match()). can store regression results, for example

consider unique %>% sort %>% toString/paste instead if it's just for browsing; or going to long format (see data structure)


### `fwrite` to write delimited files {#fwrite}

zzz

### Saving and loading R objects {#dt-saveload}

with Rdata, use load() or just drag it into the window

RDS

### Reading and writing other formats

For "foreign" table formats (excel, stata, etc), use google. these I/O functions will always be in active development because the formats themselves are in flux. of course, it's unsafe to use such formats for long-term data storage

details: feather is eventually for storage and transfer but is still under heavy development at this point.

### Formatting the display of columns

remember: do rounding as late as possible -- don't start during the cleaning and exploration phase

#### Writing data out

use fwrite squash for dates, etc

#### Saving and reloading data

RDS, dput and needing to fix pointers -- FAQ 5.3

saving sets of tables (ref organizing data) and loading it into an environment

review Gavin's post and comments below http://www.fromthebottomoftheheap.net/2012/04/01/saving-and-loading-r-objects/ 


## Exploring data {#exploring-data}

range, summary

Henk Harmsen on "ergonomics" https://rpubs.com/carbonmetrics/datatable_ihub

`e_bad = DT[, var, by=e_id][var==TRUE, e_id]; DT[.(e_id)] (simpler with having)`

`DT[, .N, by=e_id][order(-N)]`

creating temp vars, printing with []

DT[, do_stuff, j] %>% print(nrow=Inf) with magrittr (warning about performance hit in real nonexploratory code)

hist, hexbin

cor, cormat

x11() for multiple windows

eyeball-exploring models: curve

qqplot etc for the statistically literate, and other tools for ML people

### Browsing loaded objects {#browse-env}

ls()

rm()

tables()


### Subsetting {#dt-subset}

#### Selecting by value

This is analogous to a SQL WHERE clause or a Stata `if` clause.

```{r dt-sub-pre}
# example data
DT = data.table(Titanic)
```

```{r dt-sub}
DT[ Class == "Crew" & N > 100 ]
```

This syntax, `DT[i]`, is the subsetting task needed in the vast majority of cases. The rest of \@ref(dt-subset) can safely be skipped the first time through; it is mostly useful for reference.

#### Selecting by group statistic

This is analogous to a SQL HAVING clause.

```{r dt-sub-gstat}
DT[ , if (sum(N) < 300) .SD, by=Class ]
```

We are including the group only if it meets our condition. The structure `if (cond) x` returns `x` if the condition is true and nothing (`NULL`) otherwise, omitting those rows. `.SD` refers to the Subset of the Data associated with each `by=` group.

This syntax isn't as clean as a HAVING clause, but it is likely to improve.

#### Selecting rows within each group

```{r dt-sub-grow}
# select first row
DT[ , .SD[1L], by=Class ]
# select row(s) with highest count
DT[ , .SD[N == max(N)], by=Class ]
```

```{block2, type='rmd-details'}
**Efficient selection of rows within each group.**  The second task here is [somewhat inefficient](https://github.com/Rdatatable/data.table/issues/735) currently, with a faster workaround [provided by eddi](http://stackoverflow.com/a/16574176): `DT[DT[ , .I[N == max(N)], by=Class ]$V1]`. And [Arun provided a further improvement](http://stackoverflow.com/a/31854111), taking advantage of `gmax`, an efficient by-group `max` (see `?GForce`): `DT[DT[, max(N), by=Class], on=.(Class, N = V1)]`. The variable `.I` represents the row number in the full table, or at least it *should* do that. Currently, `.I` does [not always behave like this](https://github.com/Rdatatable/data.table/issues/1494). "V1" is the default name given to the result in `j`. 
```

#### Finding where a condition holds

To find where a condition holds:

```{r which}
cond = c("aleph", "baz", "boz") > "b"
which(cond)
```

So, it takes us from a logical index to one of positions. This can sometimes speed up computations; and is necessary when facing functions that don't take logical indices.

When using `which` on a matrix or array (see \@ref(matrix-array)), we can find out "where" in terms of row and column using the `arr.ind` option.

Beware of using `which` output as an index:

```{r bad-which}
x  = 1:5
w  = which(x^2 > 50)
x[-w]
```

We wanted to filter to where `x` meeting our condition, but instead we lost the whole vector. Straight-up logical filtering is the safer route here.

Data.table has `DT[i, which = TRUE]` syntax as well.


### Sorting

order(x, -y)

setorder

setkey

### Summaries {#dt-summary-stats}

draw from http://stackoverflow.com/documentation/data.table/3785 


#### The `summary` function

similar to stata

doesn't really return a useful object, only for printing

#### Collapsing a table

collapse

various custom summary functions

maybe mention GForce again


#### Counting missing values

mdesc, count missing per column colSums(is.na(.SD))


#### Tabulating {#counting-rows}

Tabulating or tallying 

bwDT[, .N, by=ftv]

refer to table() and plot.table()

With a continuous variable `x`, it often doesn't make sense to tally values as in the last section. However, we can specify bins and find where the values fall:

```{r cut}
set.seed(1)
n  = 1000
x  = rchisq(n, df = 10)
xc = cut(x, breaks = c(0, 5, 10, 15, 20, Inf)) # or max(x) in place of Inf
table(xc)
```

See the note in `?cut` for details on more efficient alternatives:

> Instead of `table(cut(x, br))`, `hist(x, br, plot = FALSE)` is more efficient and less memory hungry. Instead of `cut(*, labels = FALSE)`, `findInterval()` is more efficient.



### Reshaping to wide {#dcast}

draw from http://stackoverflow.com/documentation/data.table/4117

wide format is rarely useful for storing data (see structuring data)

but it is good for browsing

dcast

analogy with blacksmithing or something (re cast and melt), molten into its most flexible form, cast into a rigid shape for a particular purpose


todo cite `vignette("datatable-reshape")`



## Organizing relational tables {#structuring-data}

background: tidy data / database design

data.table(C02) might be a good example, with its redundant cols

### Reshaping to long format

todo cite `vignette("datatable-reshape")` again


### Choosing tables and keys


### Listing levels

make a "skeleton" for the data

consider igraph if the order does not matter

use CJ(col1, col2), maybe seq

if it's not cartesian, then DT[, col2, by=col1]


### Exploring levels

setdiff, union, intersect (works with data.table from 1.9.8+)

does the table have any dupes?

does the table have all levels?

stopifnot

### Collapsing {#aggregate-join}

essentially an aggregate join -- we join so that we can aggregate

maybe collapsing because we want a table aggregated at a higher level

maybe collapsing because we have "duplicates" hanging around for other reasons

unique, duplicated + fromLast

```{block2 dt-join-beware, type='rmd-caution'}
**Beware `DT[i,on=,j,by=bycols]`.** Only `by=.EACHI` works in a join. Typing other `by=` values there will cause `i`'s columns to become unavailable. This [may eventually change](https://github.com/Rdatatable/data.table/issues/733).
```

use CJ for missing levels

### Expanding

CJ + unique


use CJ for missing levels

Suppose we have a table measuring wealth by person and year.

```{r expandrows-pre, echo=1}
DT = data.table(psn_id = c(1L, 1L, 2L), year = c(1998L, 2000L, 2000L), wealth = 24:22)
DT
```

Now we want a person-year row for every every person and for every year from the lowest to the highest. We can do this with `CJ` and a standard join:

```{r expandrows, echo=1}
DT = data.table(psn_id = c(1L, 1L, 2L), year = c(1998L, 2000L, 2000L), wealth = 24:22)
DT
```


### Update joins

we join so that we can update the table on the left

As we saw in \@ref(dt-ifelse), we can do a SQL-style UPDATE like `x[i, var := ...]`:

```{r dt-join-updatejoin}
a[b, on=.(id), y := i.y ]    # create column, assign to each matched row of a 
a[!b, on=.(id), y := 999L ]  # assign to unmatched rows of a
```

The `i.*` syntax allows for unambiguously referring to columns from `i` (in case the two tables have columns with the same names). Similarly, the `x.*` prefix can be used to refer to columns of `x`.

```{block2 dt-updatejoin-beware, type='rmd-caution'}
**Beware multiple matches in an update join.** When there are multiple matches (\@ref(join-multimatch)), an update join will apparently only use the last one. [Unfortunately](https://github.com/Rdatatable/data.table/issues/2022), this is done silently. Try `b[a, on=.(id), x := i.x, verbose = TRUE ][]`. At least with `verbose` on it helpfully reports that it assigned "to 3 row subset of 2 rows."
```

### Programming data.table calls

getting into the weeds

DT[, if (.GRP == 1) {}, by=.(x,y,z)] for testing, or alternately browser()

eval-quote for constructing efficient calls

mget / get


review related options

options() %>% .[grep("datatable", names(.))]

#### Specifying columns {#program-cols}

shortcuts when entering j, by, .SDcols

a:b, .(a,b), !(whatever), -(whatever)

for .SDcols -- logical or integer instead of character (though I can't see any reason to use those)


shortcuts: .SDcols = V1:V10, .SDcols = grep("^date_", names(DT)), etc


```{r lapply-dt}
DT[, 
   lapply(.SD, max)
, by=z, .SDcols = { sapply(DT, is.numeric) }]
```

`.SDcols` is used to filter the columns that appear in `.SD`, the subset of data available in `j`. 

### Bad aggregations

todo -- i prolly don't need a section; there are countless ways to format data badly. but i do want to warn about by=1:nrow(DT) et al... for now, i'll put a warning block in the agg section higher up

Suppose we have data like ...

```{r dt-badagg-pre, echo=1}
badDT = data.table(col = c("A","A","B"), A = 1:3, B = -1:1)
badDT
```

Values in the `col` column refers to the other columns. We might want to do things like ...

```{r dt-badagg, eval=FALSE}
# select the column using col
badDT[, .SD[[col]], by=1:nrow(badDT)]
# take the larger column
badDT[, max(unlist(.SD)), by=1:nrow(badDT), .SDcols=c("A","B")]
```

Or we might have data like ...

```{r dt-badagg2-pre, echo=1}
badparDT = data.table(parent = c("A1","A2","B1"), kid1 = c("a","a","b"), kid2 = c("d", "d", NA), kid3 = c("f", "f", NA))
badparDT
```

Parent names consist of the family code plus a number; and kids are arrayed on columns, with as many such columns as are needed for the largest family. We might want to do things like ...

```{r dt-badagg2, eval=FALSE}
# count kids per parent
badDT[, .SD[[col]], by=1:nrow(badDT)]
# count parents per family
badDT[, max(unlist(.SD)), by=1:nrow(badDT), .SDcols=c("A","B")]
```


While such code might technically run, 

... split this section into iterating over columns and iterating over groups. explain that by= will take any appropriate-length vector, warn about by=1:nrow(DT)





